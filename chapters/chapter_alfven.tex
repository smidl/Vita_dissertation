\chapter{Practical anomaly detection: plasma physics}

Chirping Alfv\'en eigenmodes (AE) were observed at the COMPASS tokamak. They are believed to be driven by runaway electrons (RE) and as such, they provide a unique opportunity to study physics of non-linear interaction between RE and electromagnetic instabilities, including important topics of RE mitigation and losses. On COMPASS, they can be detected from spectrograms of certain magnetic probes. So far, their detection required a lot of manual effort since they occur rarely. We strive to automate this process using machine learning techniques based on generative neural networks. We present two different models that are trained using a smaller, manually labeled database and a larger unlabeled database from COMPASS experiments. On a number of experiments, we demonstrate that our approach is a viable option for automated detection of rare instabilities in tokamak plasma.

\section{Introduction}

Alfv\'en eigenmodes (AE) present a magnetic instability that appears during the experimental operation of tokamaks. The presence of fast energetic particles in tokamak plasmas can lead to destabilization of the shear Alfv\'en waves by resonance of particle velocity with Alfv\'en velocity of the plasma. These Alfv\'enic instabilities can, in turn, degrade confinement of the energetic particles and thus lower plasma performance and possibly endanger plasma-facing components of the tokamak\cite{mett1992kinetic}. Alfv\'en waves are typically excited by fast ions generated by auxiliary plasma heating or fusion reactions, however, magnetic measurements on the COMPASS tokamak\cite{markovic2017alfven} and DIII-D\cite{lvovskiy2019observation} have recently revealed Alfv\'enic modes that are driven by runaway electrons (RE). Since the presence of RE constitutes a high risk for a fusion reactor, non-linear interaction of RE and AE may be of high interest for topics of RE mitigation and study of their losses. Moreover, the presence of AE in the plasma offers a diagnostic opportunity. On JET, measurement of AE is used to compute equilibrium parameters such as the safety factor\cite{sharapov2001mhd}.

There are multiple types of AE with different characteristics. A specific type of AE, driven by RE and with chirping characteristics, is believed to be present at COMPASS\cite{markovic2015alfven, melnikov2015quasicoherent, markovic2017alfven} and similar chirping modes were also observed on the DIII-D machine\cite{lvovskiy2019observation}. On COMPASS\cite{panek2015status}, the frequency of the detected modes is in the range 0.5--2 MHz, scaling with plasma density and its profile, safety factor, magnetic field and shape of the plasma\cite{markovic2017alfven}. The mode has a bursty character. It typically appears after a sawtooth crash, together with increased RE losses detected by measurement of hard X-ray radiation\cite{markovic2017alfven}. The frequency chirp itself takes $\sim$1 ms and the frequency can chirp both up and down by $\sim$0.1 MHz. Change of RE distribution function during the frequency chirp was directly measured on DIII-D\cite{lvovskiy2019observation}.   

Up to now, spectrograms on COMPASS were labeled manually by experts. However, the rate of occurrence of chirping modes in COMPASS is relatively low (estimated to be on the order of $10^{-3}$ in terms of shots) and there are multiple measurement probes from which a spectrogram can be computed. Therefore, a lot of spectrograms have to be combed through to find an experiment during which a chirping mode has occurred. This means that to collect enough experimental data for further analysis a large amount of manual labor is required. In this work, we will try to use the available labeled spectrograms (both with and without a chirping mode) and the large unlabeled database (coming from over 15000 COMPASS discharges) to train models that would enable automatic identification of spectrograms that contain a chirping AE.

To this end, two approaches based on generative neural networks have been implemented. Generative models based on the Variational Autoencoder (VAE) paradigm\cite{kingma2013vae} have been used because they are powerful estimators of high-dimensional distributions, suitable for modelling image data. They do not require labels for training which is a limiting factor for classification neural networks which overfit when not supplied with enough labeled data. Also, VAEs possess an ability to produce a low dimensional representation of target data, which proved to be useful for our task as well. 

In the following section, the basics of probabilistic autoencoders will be given. Then, the details of experimental data, implementation and network architecture will be described together with the details concerning the experimental setup. Finally, the experimental results will be shown and their implications discussed.

\section{Chirping modes on the COMPASS tokamak}
\begin{figure}[t]%[!htbp]
  \centering
  \includegraphics[scale=0.7]{data/chapter_alfven/overview.png}
  \caption{COMPASS shot 10870. Raw U-probe signal is in the upper plot. The corresponding spectrogram is below that. The score is the output of the proposed algorithm over the spectrogram at $f_0=0.9$ MHz and it is plotted third from the top. The highest peak around 1.1s corresponds to a detected chirping mode. A close-up of the spectrogram part containing the chirping mode as detected from the red part of the score plot is at the bottom. The size of the close-up is 128 $\times$ 311 pixels.}
  \label{fig:psd}
\end{figure}

Chirping modes on the COMPASS tokamak can be observed indirectly in spectrograms of a magnetic U-probe\cite{kovavrik2011u}. The raw probe signal and its spectrogram is plotted in Fig.~\ref{fig:psd}. A single spectrogram covers one experiment of average length 0.3 s and frequency range of 0--2.5 MHz. The spectrograms are of uneven size because every experiment has a different length. Also, the size of the whole spectrograms is too large for a practical training of a neural network. Therefore, for labeling, training and validation purposes, we have split spectrograms into square patches of the same size. We have chosen the size of a single spectrogram patch to be $128 \times 128$ pixels which covers 6.54 ms in the time axis and 0.62 MHz in the frequency axis and which is a feasible input size for current convolutional neural network architectures. It is also enough to capture most of a typical chirping mode as can be seen in the bottom plot in Fig.~\ref{fig:psd}. For all future purposes, positively labeled data are those spectrogram patches that were labeled as containing a chirping mode, while negatively labeled patches do not.

\section{Model structure}
 This section contains a brief theoretical background on Variational Autoencoders and their variants. This theory will be used in the construction of two types of models -- \textit{oneclass} and \textit{two stage} which differ in the way the VAE model is trained and used. In the former, the VAE loglikelihood is used to detect out--of--distribution samples while in the former, a classifier is trained on the latent space of the autoencoder.

The VAE neural network is a generative model. The purpose of a generative model is to enable sampling from a data distribution of interest,  $p(x)$, where $x \in \mathcal{X}$ is a sample from the data distribution. Since the data space $\mathcal{X}$ is usually high-dimensional (e.g. it represents images of high definition) and all that is available for training is a finite set of samples from $p(x)$, we cannot express it in a closed form and sample from it directly. Therefore the problem of sampling is moved to a latent space $\mathcal{Z}$ on which we define a prior distribution $p(z)$ that we can easily sample from (e.g. $\mathcal{N}(0,1)$). A generative model then serves the purpose of approximating the mapping $f: \mathcal{Z} \rightarrow \mathcal{X}$ such that $f(p(z)) \approx p(x) $, i.e. sampling in the latent space is equivalent to that in the data space. Also, it provides us with an approximation of $p(x)$ which can be used to test for out-of-distribution samples. In our case, $p(x)$ that is to be approximated is the distribution of the $128\times 128$ spectrogram patches.

% include a picture of an autoencoder here
\subsection{Generative autoencoders}
A generative autoencoder consists of two main parts --- an encoder and a decoder --- which are two neural networks which can be denoted as mappings $e_{\phi}:\mathcal{X} \rightarrow \mathcal{Z}$ and $d_{\theta}:\mathcal{Z} \rightarrow \mathcal{X}$, where $\lbrace \phi, \theta \rbrace$ are trainable parameters (weights) of the neural network. The decoder parametrizes the generative distribution $p_{\theta}(x|z)$, which means that it is used to compute the parameters (e.g. mean and variance) of $p_{\theta}(x|z)$ as a function of $z$. Likewise, the encoder parametrizes the encoding distribution $q_{\phi}(z|x)$. 

The whole process of passing a sample $x$ through the network during training is following: $z$ is sampled from encoding distribution $q_{\phi}(z|x)$ whose parameters are obtained from $e_{\phi}(x)$. This is then passed to the decoder $d_{\theta}(z)$ which produces parameters of $p_{\theta}(x|z)$ from which a reconstruction $\hat{x}$ is sampled. Through this, the generative autoencoder maximizes the probability of each sample obtained through the generative process with respect to the available data

\begin{equation}
  p_{\theta}(x) = \int_{\mathcal{Z}} p_{\theta}(x|z) p(z) dz.
  \label{eq:gen}
\end{equation}

The general expression for the training loss that minimizes~\eqref{eq:gen} can be expressed in accordance with\cite{mescheder2017adversarial,tolstikhin2017wasserstein} as

\begin{equation}
  \mathcal{L}(x,\theta,\phi)=\inf_{q_{\phi}(z|x)}\mathbb{E}_{x\sim p(x)}\mathbb{E}_{z\sim q_{\phi}(z|x)}\left[\ln p_{\theta}(x|z))\right]+\lambda \Gamma(p(z),q_{\phi}(z)),
  \label{eq:WAE_loss}
\end{equation}

where $\mathbb{E}_p \left[ . \right]$ denotes the expected value, $\lambda > 0$ is a scaling parameter (or hyperparameter), $\Gamma(.,.)$ is a divergence measure between two probability distributions and $q_{\phi}(z) = \mathbb{E}_{x\sim p(x)}\left[q_{\phi}(z|x)\right]$ is the encoding distribution marginal. The first term forces the reconstruction $\hat{x}$ to be as close to $x$ as possible so that the generated samples resemble the training data. The second term pushes the encoder distribution close to the prior. This has the effect that after training, one can generate new data by passing sample from the prior (instead from the encoder) to the decoder. This generated sample will have a high probability under $p(x)$ (look realistic) if the prior and encoding distributions are indeed close.

\begin{figure}[htpb]
\begin{center}
\begin{tikzpicture}[scale=1, transform shape]
  \vspace{-10cm}
  \node (image) at  (0,0) {\includegraphics[width=\linewidth]{data/chapter_alfven/model_structure.pdf}};
  \node (encdim) at (-4,\capy) {$\x \in \mathbb{R}^{128\times128\times1}$};
  \node (decdim) at (4,\capy) {$\hat{\x} \in \mathbb{R}^{128\times128\times1}$};
  \node (latdim) at (0, \capy) {$\z \in \mathbb{R}^{d}$};
  \node (enc) [align=left] at (\encx, .5) {encoder\\ $e_{\phi}(\x)$};%{$e_{\phi}(\x)$};
  \node (enc) [align=left] at (\encx, -.5) {Conv. +\\ Dense};
  \node (dec) [align=left] at (\decx, .5) {decoder\\ $d_{\theta}(\z)$};%{$d_{\theta}(\z)$};
  \node (dec) [align=left] at (\decx, -.5) {Dense +\\ Tr. Conv.};
\end{tikzpicture}
\end{center}
\caption{A schematic diagram of the convolutional autoencoder used for our experiments. Spectrogram patches are encoded through several convolutional\cite{lecun1989backpropagation}, maxpooling\cite{ranzato2007efficient} and dense (fully connected) layers into $d$-dimensional vectors (here $d=2$) and then decoded back with transposed convolutions and upscaling layers.}
\label{fig:ae}
\end{figure}

In our models, we will use Gaussian encoding and generating distributions, that is $q_{\phi}(z|x )= \mathcal{N}\left( z | \mu_{\phi}(x), \Sigma_{\phi}(x) \right)$ and $p_{\theta}(x|z)= \mathcal{N}\left( x | \mu_{\theta}(z), \Sigma_{\theta}(z) \right)$. Different choices of $\Gamma$ and the resulting optimization objectives are explored here. During training, the objective~\eqref{eq:WAE_loss} is minimized with respect to parameters ${\theta, \phi}$ using backpropagation and a standard optimization algorithm, e.g. Adam\cite{kingma2014adam}. A schematic diagram of a convolutional autoencoder is in Fig.~\ref{fig:ae}.

\paragraph{Kullback-Leibler divergence} 
The use of Kullback-Leibler divergence (KLD) $D_\text{KL}(q_{\phi}(z|x)||p(z))$ results in the well-known Variational Autoencoder model\cite{kingma2013vae}, where the prior $p(z)=\mathcal{N}(0,1)$ is used in order to obtain an analytical expression of the optimization objective.

\paragraph{Maximum Mean Discrepancy} 
The MMD divergence $\text{MMD}_k(q_{\phi}(z|x),p(z))$ can be also used, which requires a kernel $k$. For details see\cite{tolstikhin2017wasserstein}. Its optimization requires sampling from both latent distributions, which means that any prior that can be sampled from can be used --- even a mixture prior, whose probability density $p(z)$ consists of several modes. In our work, we have used the VampPrior~\cite{tomczak2017vae}, which enables optimization of the parameters of the components of the latent mixture, thus giving more flexibility to the overall model.

\paragraph{Jensen-Shannon divergence} 
The Jensen-Shannon divergence (JSD) $D_{\text{JS}}(q_{\phi}(z|x)||p(z))$ is a symmetric form of KLD. Its use was again demonstrated in\cite{tolstikhin2017wasserstein} where it was shown that it leads to the adversarial loss used in Adversarial Autoencoder (AAE) model\cite{makhzani2015adversarial}. It requires an additional network - a discriminator $d_\eta:\mathcal{Z}\rightarrow[0,1]$ that tries to recognize between $z$ sampled from prior and that sampled from the encoding distribution. The output of the network is a probability that the input comes from the prior $p(z)$. The discriminator is trained in tandem with the encoder so that they both improve in their tasks and the encoder learns to map the data $x$ so that they resemble samples from the prior. Again, here we have used the VampPrior.

A form of the adversarial loss is commonly used in Generative Adversarial Networks (GANs) and although it leads to generated samples of better quality, its use is known to destabilize the training process\cite{goodfellow2016nips}. This holds for AAE models as well. For this reason, we have used a combination of the MMD and the GAN loss in order to stabilize the training of encoder and the distribution it produces.

We have used a plain autoencoder\cite{vincent2010stacked} as a baseline, which optimizes the mean squared error (MSE) between input $x$ and the reconstruction $\hat{x}$
\begin{equation}
  \mathcal{L}_{\text{AE}}(x,\theta,\phi) = || x - d_{\theta}(e_{\phi}(x)) ||^2.
\end{equation}

\subsection{One-class model}
In the first model, we will use a convolutional generative autoencoder as a one-class estimator. This is an approach well known in the anomaly/outlier detection setting\cite{chandola2009anomaly,scholkopf2001estimating}. A model of choice learns a representation of one class of data and can, therefore, be used to detect out-of-class samples. It is trained either with labeled data belonging to the class of interest, or with unlabeled data which are believed to contain so little out-of-class examples that the model is robust enough to ignore them. 

A generative autoencoder can be readily used for this task if we set $p_{\theta}(x) \approx p(x)$ to be the distribution of the class of our interest. Then there are two modes of training the autoencoder. In the first mode, we model the distribution of patches that contain a chirping mode. Then the autoencoder is trained with the positively labeled data. However, this is a bit problematic since there are very few labeled patches available, therefore the neural network will very likely overfit. In the second mode, we can choose the class of interest to be of the patches that do not contain a chirping mode. This is closer to an anomaly detection formulation of the problem, as the relatively rare chirping modes are considered to be anomalous. Also, the autoencoder can be trained with unlabeled data due to the sparse occurrence of chirping modes and robustness of probabilistic neural networks\cite{an2015variational,leveau2017adversarial}, hugely increasing the number of training samples and thus the representative power of the neural network. On the other hand, this might

To decide whether a sample $x$ is in- or out-of-class, we can compute its negative loglikelihood under the generative distribution 
\begin{equation} \label{eq:llh}
  -\mathbb{E}_{q_{\phi}(z|x)}\left[\ln p_{\theta}(x|z)\right]
\end{equation}
or its approximation, the MSE between $x$ and its (sampled) reconstruction $\hat{x}$. In our experiments, we use loglikelihood~\eqref{eq:llh} since it better captures the uncertainty in the reconstruction.

\subsection{Two-stage model}
The second model is designed to make the most use of both labeled and unlabeled data. It exploits the ability of generative autoencoders to produce low dimensional uncorrelated representation of high dimensional image data. It consists of two stages. The first stage is a convolutional generative autoencoder trained with unlabeled data. Its task is to learn the general topology of the input space and encode input data. The second stage is a classifier that is trained on encoded labeled data. Through the use of MMD or $\text{JS}_D$ measures and VampPrior, we can enforce separation of the encoded data into clusters that contain similar inputs, which makes the task of the classifier easier. Two different classifiers were tested.


\paragraph{kNN} The kNN algorithm for classification\cite{deng2016efficient} was trained using the labeled training data. In this setting, an unlabeled sample is given a score based on the average label of its $k$-nearest neighbors. The more neighboring training samples are labeled as positive, the higher the score.

\paragraph{GMM} A Gaussian Mixture Model (GMM)\cite{huang2005gaussian} with $M$ components was fitted on the latent representations of both labeled and unlabeled training data. Afterward, we determine one or more components of the mixture into which the positively labeled training samples are most likely to be projected via the encoder. Then, for a new sample, the score is the (average) loglikelihood of the sample in the anomalous components. 

\section{Experimental setup}

\subsection{Data}
Every spectrogram was divided into patches of size $128\times128$ pixels. Out of 40 pre-processed spectrograms, 370 non-overlaping patches were extracted and labeled. This results in a labeled training dataset $\left\{ X_{l},Y\right\} ,X_{l}=\left\{ x_{i}\right\} _{i},x_{i} \in \mathbb{R}^{128\times128\times1}, Y=\left\{ y_{i}\right\} ,y_{i}\in\left\{ 0,1\right\}$ of samples $X_{l}$ and labels $Y$, where $Y=1$ if a patch contains a chirping mode. Also, an unlabeled database $X_{u}$ of 330000 patches coming from 2000 spectrograms was created. 

Training of the one-class model was done both with labeled positive spectrograms and on the large unlabeled dataset. In the first case, the 50\% of positive spectrograms were used for training and the rest together with the unlabeled ones for testing. Also, training patches were randomly shifted and noise was added to them so that there were a total of $10^4$ training samples. In the second case, all of the labeled data was only used for testing. 10 different training and testing datasets were created this way for cross-validation purposes.

For training of the two-stage model, we have split the labeled dataset to training/testing subsets with the ratio 80/20. Again, this splitting was done randomly a total of 10 times.

\subsection{Model architecture and hyperparameters}

\begin{table}
\centering

\captionsetup[subtable]{position = below}
\captionsetup[table]{position=below}

\begin{subtable}{0.4\linewidth}
  \centering
  \begin{tabular}{c | c}
     parameter & values\tabularnewline
    \hline 
    $\gamma$ & $\left\{ 10^{0},10^{-1},10^{-2}\right\} $\tabularnewline
    $\lambda,\lambda_{1},\lambda_{2}$ & $\left\{ 10^{1},10^{0},10^{-1}\right\} $\tabularnewline
    $d$ & $\left\{ 8, 128,256\right\} $\tabularnewline
    
  \end{tabular}
  \caption{One-class model.}
  \label{tab:1c_params}
\end{subtable}
\hspace*{4em}
\begin{subtable}{0.4\linewidth}
  \centering
  \begin{tabular}{c  c | c}
      & parameter & values\tabularnewline
    \hline 
    \multirow{4}{*}{1st stage} & $N$ & $\left\{ 1,2,4,8\right\} $\tabularnewline
    & $\gamma$ & $\left\{ 10^{0},10^{-1},10^{-2}\right\} $\tabularnewline
    & $\lambda,\lambda_{1},\lambda_{2}$ & $\left\{ 10^{1},10^{0},10^{-1}\right\} $\tabularnewline
    & $d$ & $\left\{ 2,16,32,64\right\} $\tabularnewline
    \hline
    \multirow{2}{*}{2nd stage} & $k$ & $\left\{ 1,3,\ldots,31\right\} $\tabularnewline
    & $M$ & $\left\{ 2,4,6,8\right\} $\tabularnewline

  \end{tabular}
  \caption{Two-stage model.}
  \label{tab:2s_params}
\end{subtable}
\caption{Overview of model hyperparameters. $\lambda_{1}$ and $\lambda_{2}$ are scaling parameters for the combination of MMD and adversarial loss.}
\end{table}

\paragraph{One-class model}
The architecture for the one-class encoder was 2 or 3 convolutional layers with (32,64) or (32,32,64) channels and kernel size of 5. Each convolutional layer was followed by a maxpooling layer which downscaled the image by a factor of 2. Then a dense layer produced the final encoding into a $d$-dimensional latent space. The decoder mirrored the encoder architecture with transposed convolutions in place of maxpooling layers. ResNet\cite{he2016deep} type residual blocks were used to speed up and stabilize the training. The hyperparameter values over which we have optimized trained models are in Tab.~\ref{tab:1c_params}.
Parameter $\gamma$ denotes the scaling parameter of the inverse multiquadratics (IMQ) kernel\cite{gorham2017measuring}.

\paragraph{Two-stage model}
The basic encoder architecture was the following: 3 convolutional layers with (16,16,32) channels and kernel size of 3, (2,2,1) downscaling ratios via maxpooling, followed by two dense layers of width (256,$d$) where $d$ is the dimension of the latent space. The decoder mirrored this architecture. Also, batch normalization\cite{ioffe2015batch} was used. The hyperparameter values over which we have optimized are in Tab.~\ref{tab:2s_params}. The number of components in the used prior is denoted by $N$. 

The base architecture of the two models is slightly different. We have experimented with different architectures prior to the hyperparameter optimization and found that for the different tasks, different architectures provide better results. This is probably due to the fact that both models have a different objective - the one-class model requires precise reconstruction, while the two-stage is evaluated based on the shape of latent space. Ideally, we would include architectures as a tunable parameter, but that would require a level of computational power that was not available to us.

Both models use ReLu\cite{hahnloser2000digital} activation and were optimised with the RMSProp optimizer with learning rate $10^{-4}$. For a single optimization iteration, batches of 128 patches were used. We have implemented all the models in the Julia language\cite{bezanson2017julia} and trained them on TITAN V Nvidia GPU with 12 GB of memory. 

\section{Results}

In this section, the result of experiments with one-class and two-stage models are going to be examined and compared. Also, we will discuss the importance of an appropriate train/test splitting strategy that was used in our experiments in order not to obtain overly optimistic model performance estimates.

\subsection{Output evaluation}

\begin{figure}
\begin{centering}
\includegraphics[scale=0.5]{data/chapter_alfven/anomalies.png}
\par
\end{centering}
\caption{Examples of spectrogram patches identified as containing a chirping
mode.}
\label{fig:alfven_patches}
\end{figure}

In case our framework was implemented in a production environment, the working scenario would be the following. A set of experiments to be analyzed would be selected. Then, the needed signals would be extracted, spectrograms computed and divided into patches of appropriate size. These would be fed to a trained model that would produce scores to enable ranking of the patches. Since this would produce hundreds, maybe thousands of patches and scores, the operator would ideally only want to go through a few with the highest score. In Fig.~\ref{fig:alfven_patches} we show the output of such procedure --- 4 patches with the highest score, out of which 3 contain a chirping mode. It illustrates that even though the neural network encoding might be powerful, it is still basically a black box model and we need to be very careful in its evaluation. Because of this, we evaluate the model performance by computing AUC (area under the receiver operating curve), which is a standard measure for binary classification problems, and also by precision@$k$ score, which is the precision at the $k$-highest scoring samples. 

\subsection{One-class model optimization}

\begin{table}
  \centering
  \input{data/chapter_alfven/one_class_results}
  \caption{Results of optimization of the one-class model. Target class differences are described in the experimental setup section. No divergence is a plain autoencoder with MSE training objective.}
  \label{tab:one_class}
\end{table}

The hyperparameter optimization routine resulted in hundreds of trained models. To select the best one, the AUC and precision@50 measures were computed on a testing dataset. Then, for a set of fixed hyperparameter values, these were averaged over 10 cross-validation test-train splits. The best results for a combination of target class and used divergence based on these measures are reported in Tab.~\ref{tab:one_class}. Clearly, it seems that modeling the distribution of chirping mode spectrograms is more difficult than vice versa with the exception of KLD, which completely fails. Also, the precision in top samples is very low in the Alfv\'en target class. Surprisingly, a plain autoencoder achieves results almost comparable to other models. In Fig.~\ref{fig:roc_prc} are the ROC (receiver operating characteristic)\cite{fawcett2006introduction} and PR (precision-recall)\cite{boyd2013area} curves of the single best performing one-class models as well as the two-stage models.

\begin{figure}
\begin{centering}
\includegraphics[scale=0.6]{data/chapter_alfven/roc_prc.pdf}
\end{centering}
\caption{ROC and PR curves of selected models.}
\label{fig:roc_prc}
\end{figure}

\subsection{Two-stage model}

\begin{table}
\centering
\input{data/chapter_alfven/two_stage_results}
\caption{Results of hyperparameter tuning of the two-stage model across 10 cross-validation splits.}
\label{tab:alfven_results}
\end{table}

Here, we evaluate the performance of the two-stage model. The methodology of hyperparameter optimization via cross-validation is similar to that used for the one-class model.  The best average result across 10 splits for different combinations of stage one divergences and stage two classifiers are reported in Tab.~\ref{tab:alfven_results}. The simple kNN model is superior to the GMM approach. Also, MMD regularization seems to produce the best results. We might speculate that this might be due to the improved ability to produce a well-separated encoding enforced by the used prior. Again, in Fig.~\ref{fig:roc_prc} see the ROC and PRC curves for the single best two-stage models.

A question one might ask is whether the autoencoding is truly necessary. In the end, we are doing a projection from $d=128\times128=16384$ dimensional picture space into at most $d=64$ dimensional latent space which must naturally lead to a loss of information. As shown in Fig.~\ref{fig:patches_latent}, where $d=8$, the autoencoder is able to identify the difficult nonlinear correlations and improve the performance of a subsequent second stage kNN model. The compression is clearly necessary for overcoming the curse of dimensionality which implies that L$_2$ distance degenerates in large dimensions. An alternative approach to overcoming the issue of large input dimension might be to train a classification convolutional neural network, which does the compression by its nature. We have not chosen to go this path however since we believe that such a network would be highly susceptible to overfitting since it requires a lo of labeled data which is not available to us. Instead, we have tried to overcome this in the two-stage model by learning the compression from all the available unlabeled data.

\subsection{Influence of the train/test splitting methodology}

\begin{figure}
\centering
\includegraphics[scale=0.45]{data/chapter_alfven/split_patches.pdf}
\includegraphics[scale=0.45]{data/chapter_alfven/split_spectrograms.pdf}
\caption{kNN fits for different values of $k$. The red line and band show the mean and one standard deviation bands of the resulting AUC values when kNN is fitted to the original vectorized images. The input space dimensionality is $d=16384$. The blue dashed line and band are the same quantities for a $d=8$ dimensional representation by a first stage model. On the left, the training and testing splits were done on the level of individual patches, leading to a better performance and less variance. On the right, the split was done on the level of the original spectrograms, which is a more realistic scenario. The standard deviation and mean were computed from 10 random splits.}
\label{fig:patches_latent}
\end{figure}

At first, the splitting of testing and training labeled patches was done on the level of patches, without any regard for the spectrogram/experiment that the patch came from. It was assumed that the labeled chirping modes are homogeneous across the spectrograms. However, this turned out not to be true. Therefore, the train/test splits were done on the level of spectrograms which were then subsequently divided into patches. See Fig.~\ref{fig:patches_latent} where on the left side, the AUC curves for different values of $k$ of the kNN model are for the case when the data split was done on the level of patches. The blue line that is the result of kNN fit peaks at $k=3$. On the other hand, there is no such peak on the right side of the figure, where splitting was done on the level of spectrograms. This indicates that the positively labeled patches in a single spectrogram are much more similar to each other than to those in different spectrograms, as only a relatively low number of neighbors is sufficient for optimal performance. Also, the variance of the right side plots is much higher, again indicating larger differences across spectrograms. If we continued with the splitting on the level of patches, we would have a biased and too optimistic estimate of performance before putting the framework into production.

\section{Conclusion}
Our task was identification of anomalous phenomena --- chirping Alfv\'en modes --- in graphical representations of signals measured during the operation of a tokamak. To this end, we have proposed two models based on generative autoencoders. The first model learned the distribution of normal data and identified chirping modes as out-of-class samples of this distribution. The second model implemented a two-stage learning approach. A regularized convolutional variational autoencoder trained on unlabeled data was successfully combined with a classifier trained with a smaller labeled dataset. It has been shown that both models are viable options in chirping mode identification, although the latter one proved to be superior.

We have also shown the need for proper cross-validation splitting of data in the evaluation phase and outlined the need for careful evaluation in order for the model to be useful in real-world application. However, this is still work in progress. We mentioned the need to use a more appropriate evaluation measure that reflects the operational conditions. Furthermore, so far we have only used spectrograms from a single U probe, but there are about 40 more magnetic diagnostics that could be potentially used for this task, e.g. their spectrograms/correlograms could be added as an additional input channel. Finally, a more thorough evaluation of the contemporary experimental results is needed for the understanding of the framework behavior and to be applicable to COMPASS operation, most likely through the expansion of the labeled dataset. 
