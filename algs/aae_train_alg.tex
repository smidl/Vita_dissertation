\begin{algorithmic}[1]
\Require{A training set $X=\lbrace x_j \rbrace \in \mathbb{R}^d$, maximum number of iterations $I\in\mathbb{N}$, batchsize $L \in \mathbb{N}$, the number of decoder samples $M$, regularization coefficient $\lambda \in \mathbb{R}$.}
\State $\phi,\theta, \eta \gets $ Initialize parameters
\State{$i \gets $ Iteration counter}
\While{$i<I$ or $\phi,\theta,\eta$ are not converged}
	\State{$X_L \gets$ A random batch of $L$ samples from $X$}
	\State{$Z_L \gets$ A random batch of $L$ samples from $p(z)$}
	\State{$\tilde{Z}_L \gets \lbrace\tilde{z}_j \sim q_{\phi}(z|x_j), x_j \in X_L \rbrace$ samples from $q_{\phi}(z|x)$}
	\State{$\tilde{X}_L \gets \lbrace\tilde{x}_j \sim p_{\theta}(x|\tilde{z}_j), \tilde{z}_j \in \tilde{Z}_L \rbrace$ samples from $p_{\theta}(x|z)$}
	\State{$l_d \gets \frac{1}{L}\sum_{j=1}^L \mathcal{L}_d(z_j,\tilde{z}_j,\eta), z_j \in Z_L, \tilde{z}_j \in \tilde{Z}_L$}
	\State$\eta \gets $ Update parameters with gradients $\nabla_{\eta} l_d$ to maximize $l_d$
	\State$l_{ae} \gets \frac{1}{L}\sum_{j=1}^L \mathcal{L}_{ae}(x_j,\tilde{x}_j,\tilde{z}_j,\theta,\phi), x_j \in X_L, \tilde{x}_j \in \tilde{X}_L, \tilde{z}_j \in \tilde{Z}_L$
	\State{$\phi,\theta \gets $ Update parameters with gradients $\nabla_{\theta,\phi} l_{ae}$ to minimize $l_{ae}$}
	\State{$i \gets i+1$}
\EndWhile
\State{\textbf{return} encoder $q_{\phi}(z|x)$, decoder $p_{\theta}(x|z)$, discriminator $d_\eta(z)$}
\end{algorithmic}